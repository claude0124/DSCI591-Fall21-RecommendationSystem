{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6bad66b-b97b-4b9f-9056-911a31890235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import feedparser, urllib, json, os, re, requests\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5cdaf06-1594-4766-a520-fadd8ecb9014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#soup.find_all('h3', {'class': 'StyledHeading-sc-1rdh4aw-0 haGwpv'})\n",
    "\n",
    "def grab_ratings(brand_page_soup):\n",
    "\n",
    "    #regrex patterns\n",
    "    rate_pat = re.compile('\\d')\n",
    "    cost_pat = re.compile('[$]')\n",
    "\n",
    "    brand_ratings = defaultdict(dict)\n",
    "    brand_metrics = brand_page_soup.find_all('span', {'class': 'StyledText-sc-1sadyjn-0 ccIhDL'})\n",
    "\n",
    "    #get second item, first item is the aggregated \"rating\", then count the number of '$' to rank it\n",
    "    brand_cost = brand_page_soup.find_all('span', {'class': 'StyledText-sc-1sadyjn-0 kkXGYR'})[1].text\n",
    "    brand_cost_rank = len(re.findall(cost_pat, brand_cost))\n",
    "\n",
    "\n",
    "    for i, metric in enumerate(['plant', 'people', 'animals']):\n",
    "        rating = re.findall(rate_pat, brand_metrics[i].text)\n",
    "        if not rating:\n",
    "            brand_ratings[metric] = None\n",
    "        else:\n",
    "            rating_dec = int(rating[0])/int(rating[1])\n",
    "            brand_ratings[metric] = rating_dec\n",
    "    \n",
    "    return(brand_cost_rank, brand_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec163bfd-f1e7-495b-b6d9-b3ff0a26a8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAIN \"TOPS\"\n",
    "\n",
    "tops_site = 'https://directory.goodonyou.eco/categories/tops'\n",
    "hdr = {'User-agent': 'Mozilla/5.0'}\n",
    "req = urllib.request.Request(tops_site,headers=hdr)\n",
    "\n",
    "tops_page = urllib.request.urlopen(req)\n",
    "tops_soup = BeautifulSoup(tops_page, 'html.parser')\n",
    "entire_page_meta = json.loads(tops_soup.script.string)\n",
    "\n",
    "all_brands = entire_page_meta['props']['pageProps']['category']['brands']\n",
    "\n",
    "tops_brand_page = {'https://directory.goodonyou.eco/brand/'+i['id'] for i in all_brands}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6177803d-2bca-4e91-bcd4-243798bde9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEPPING INTO EACH BRAND PAGE\n",
    "t_shirt_brand_dict = {}\n",
    "for brand_site in tops_brand_page:\n",
    "    \n",
    "    req2 = urllib.request.Request(brand_site,headers=hdr)\n",
    "\n",
    "    try:\n",
    "        page2 = urllib.request.urlopen(req2)\n",
    "    except:\n",
    "        t_shirt_brand_dict[brand_site] = 'NOT FOUND'\n",
    "        continue\n",
    "  #  page2 = urllib.request.urlopen(req2)\n",
    "    soup2 = BeautifulSoup(page2, 'html.parser')\n",
    "    brand_cost, brand_rate = grab_ratings(soup2)\n",
    "    \n",
    "    \n",
    "    brand = json.loads(soup2.script.string)\n",
    "    brand_meta = brand['props']['pageProps']['brand']\n",
    "    t_shirt_check=False\n",
    "    for cat_i in brand_meta['categories']:\n",
    "        if cat_i['name']=='T-Shirts':\n",
    "            t_shirt_check = True\n",
    "\n",
    "    if t_shirt_check:\n",
    "        pass\n",
    "    else:\n",
    "        continue\n",
    "    t_shirt_brand_dict[brand_meta['id']] = {'link':brand_site, 'cost':brand_cost, 'rating':brand_rate, 'extra_meta':brand_meta}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59ce6003-72e2-4ce6-a876-15b6ae8500c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that output.json must already exist at this point\n",
    "with open('data/brand_data.json', 'w+') as f:\n",
    "    # this would place the entire output on one line\n",
    "    # use json.dump(lista_items, f, indent=4) to \"pretty-print\" with four spaces per indent\n",
    "    json.dump(t_shirt_brand_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1e24a24-7939-49a6-8bbe-ad579c7547d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/brand_data.json', 'r') as f:\n",
    "    loaded = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720f5d54-7641-46a8-b798-c6b578bbcda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#brand name: {link:(http://...), cost:(1,2,3,4), ratings{planet:(1-5), 'people':(1-5), 'animals':(1-5)}, additional metadata: {...}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8eeac7-fe88-470d-883a-d4e34653f0ac",
   "metadata": {},
   "source": [
    "# Trying to go deeper into other sites in a general way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0cdc30d6-79d8-4027-af2f-f4003a76fbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_site = loaded['sandro']['extra_meta']['website']\n",
    "hdr = {'User-agent': 'Mozilla/5.0'}\n",
    "req = urllib.request.Request(tst_site,headers=hdr)\n",
    "\n",
    "tops_page = urllib.request.urlopen(req)\n",
    "tops_soup = BeautifulSoup(tops_page, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "784a1e8c-ce9e-4042-8ae7-515aa4eee85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Women\n",
      "Clothing\n",
      "Shoes\n",
      "Bags & Accessories\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Men\n",
      "Clothing\n",
      "Shoes\n",
      "Accessories\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FAQ\n",
      "Contact Us\n",
      "Our Stores\n",
      "Women\n",
      "Men\n",
      "Australia\n",
      "Austria\n",
      "Belgium\n",
      "China\n",
      "France\n",
      "Germany\n",
      "Greece\n",
      "Hong Kong SAR / Macau SAR\n",
      "Ireland\n",
      "Italy\n",
      "Luxembourg\n",
      "Netherlands\n",
      "Portugal\n",
      "Russia\n",
      "Singapore\n",
      "Spain\n",
      "Switzerland\n",
      "Taiwan Region\n",
      "United Kingdom\n"
     ]
    }
   ],
   "source": [
    "for link_i in tops_soup.find_all('a'):\n",
    "    print(link_i.text)\n",
    "    \n",
    "    \n",
    "    #, {'text':'Women'})#[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f1031e-728b-4f3f-91d0-20bea7624425",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
