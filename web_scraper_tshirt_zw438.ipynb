{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Set up environment ===\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "urllib3.disable_warnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "# connect to the target website\n",
    "\n",
    "baseurl = 'https://www.uniqlo.com'\n",
    "\n",
    "# It is important to list the exact browser version in the headers for successful implementation of request.get\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36', \"Upgrade-Insecure-Requests\": \"1\",\"DNT\": \"1\",\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\"Accept-Language\": \"en-US,en;q=0.5\",\"Accept-Encoding\": \"gzip, deflate\"\n",
    "}\n",
    "\n",
    "tshirts_webpage_women = 'https://www.uniqlo.com/us/en/women/tops/t-shirts'\n",
    "tshirts_webpage_men = 'https://www.uniqlo.com/us/en/men/tops/t-shirts'\n",
    "tshirts_webpage_kids = 'https://www.uniqlo.com/us/en/kids/tops/t-shirts'\n",
    "\n",
    "tshirt_list_kids = requests.get(tshirts_webpage_kids, verify = False, headers = headers)\n",
    "print(tshirt_list_kids.status_code)\n",
    "    \n",
    "soup = BeautifulSoup(tshirt_list_kids.content, 'html.parser')\n",
    "section_list = soup.find_all('div', class_ = 'subcategory-section row')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Sleeve:  5\n",
      "=======Long Sleeve-kids fleece high-neck long-sleeve t-shirt=======\n",
      "=======Long Sleeve-kids u striped crew neck long-sleeve t-shirt=======\n",
      "=======Long Sleeve-girls frill sleeve long-sleeve t-shirt=======\n",
      "=======Long Sleeve-kids soft touch crew neck long-sleeve t-shirt=======\n",
      "=======Long Sleeve-kids soft touch mock neck long-sleeve t-shirt=======\n",
      "Short Sleeve:  5\n",
      "=======Short Sleeve-kids u airism cotton crew neck t-shirt=======\n",
      "=======Short Sleeve-kids cotton color crew neck short-sleeve t-shirt=======\n",
      "=======Short Sleeve-kids cotton color crew neck short-sleeve t-shirt=======\n",
      "=======Short Sleeve-kids dry-ex crew neck short-sleeve t-shirt=======\n",
      "=======Short Sleeve-kids cotton color crew neck short-sleeve t-shirt=======\n"
     ]
    }
   ],
   "source": [
    "# ==== Data definition ====\n",
    "\n",
    "# saving scrapted data into lists of Dictionary\n",
    "#tshirts_info_women = []\n",
    "#tshirts_info_men = []\n",
    "#tshirts_info_kids = []\n",
    "\n",
    "# initialize the dictionary of t-shirt information\n",
    "tshirt_info = {\n",
    "    'name': 'name',   # product name of the tshirt\n",
    "    'link': 'link',  # link to the main page\n",
    "    'price': 'price',  # retail price\n",
    "    'sizes': 'sizes',  # available sizes of this tshirt\n",
    "    'rating': 'avg_rating',  # average rating value\n",
    "    'review_count': 'review_count', # number of reviews/ratings\n",
    "    'materials': 'materials',  # material composition\n",
    "    'care_advice': 'care_advice',  # care advice for this tshirt\n",
    "    'origin': 'origin',  # imported or domestic\n",
    "    'IsNew': '',\n",
    "    'colors': [],  # list of available colors\n",
    "    'color_sizes': [],  # list of relevant size ranges of the available colors\n",
    "    'thumb_links': [],  # list of links to the relevant thumb images of the available colors\n",
    "    'category': 'category',  # style category of this tshirt\n",
    "    'category_link': 'category_link',  # link to the main page of the relevant style category (similar goods)\n",
    "    'summary': 'summary',  # feature summry from the merchant\n",
    "    'details': [],  # list of detailed descriptions from the merchant\n",
    "    'scrape_time': 'scrape_time',  # finish time of scraping\n",
    "}\n",
    "\n",
    "# ==== Scraping data ====\n",
    "\n",
    "# 1st Level: tshirt subcategory (Essential, Fashion ...): name, item number and link of webpage\n",
    "for section in section_list:\n",
    "    category_name = section.find(\"a\", href = True).get_text().split('\\n')[1]  # style category\n",
    "    category_link = baseurl + section.find(\"a\", href = True)['href']\n",
    "    product_list = section.find_all(\"div\", class_ = \"product-tile\")\n",
    "    subcat_number = len(product_list)\n",
    "    print(category_name + ': ', subcat_number)\n",
    "    \n",
    "    # 2nd Level: each individual product under the current subcategory\n",
    "    for product in product_list:\n",
    "        \n",
    "        # basic information of the current product\n",
    "        tshirt_info['name'] = product.find(\"a\", class_ = \"link\").get_text()\n",
    "        tshirt_info['link'] = baseurl + product.find(\"a\", class_ = \"link\")['href']\n",
    "        tshirt_info['IsNew'] = product.find(\"span\", class_ = \"rightBadge\").get_text().split('\\n')[1].strip()\n",
    "        tshirt_info['sizes'] = product.find(\"span\", class_ = \"swatch-size-values\").get_text()\n",
    "        tshirt_info['price'] = product.find(\"span\", class_ = \"value\").get_text().split('\\n')[3].strip()\n",
    "        tshirt_info['category'] = category_name\n",
    "        tshirt_info['category_link'] = category_link\n",
    "        \n",
    "        # collect names and thumb-image links of all the available colors of the current product\n",
    "        color_names = []  # names of available colors\n",
    "        color_sizes = []  # specific size range of available colors\n",
    "        thumb_links = []  # links of relevant thumb images of available colors\n",
    "        \n",
    "        for product_color in product.find_all('img'):\n",
    "            # the first element is the default display color of the current product without links to thumb images, which should be skipped\n",
    "            if product_color.get_attribute_list('data-medium-img') != [None]:\n",
    "                color_names.append(product_color['data-medium-img'].split('\"')[7].split(',')[1].strip())\n",
    "                thumb_links.append(product_color['data-medium-img'].split('\"')[3])\n",
    "                color_sizes.append(product_color['data-size-value'])\n",
    "                \n",
    "        tshirt_info['colors'] = color_names\n",
    "        tshirt_info['color_sizes'] = color_sizes\n",
    "        tshirt_info['thumb_links'] = thumb_links\n",
    "        \n",
    "        # accessing the main page of the current product to extract rating value/count, and other attributes\n",
    "        product_mainpage = requests.get(tshirt_info['link'], verify = False, headers = headers)\n",
    "        soup_product = BeautifulSoup(product_mainpage.content, 'html.parser')\n",
    "        \n",
    "        # average rating values & review count\n",
    "        try:\n",
    "            tshirt_info['rating'] = soup_product.find(\"span\", class_=\"bvseo-ratingValue\").get_text().strip()\n",
    "        except:\n",
    "            tshirt_info['rating'] = 'unknown'\n",
    "        try:\n",
    "            tshirt_info['review_count'] = soup_product.find(\"span\", class_=\"bvseo-reviewCount\").get_text().strip()\n",
    "        except:\n",
    "            tshirt_info['review_count'] = 'unknown'\n",
    "        \n",
    "        #product_avg_fit  # 0-very small, 100-very large; 50-suitable --- inaccessible\n",
    "        #product_avg_length  # 0-very short, 100-very long; 50-suitable --- inaccessible\n",
    "        #product_avg_quality  # 0-poor, 100-perfect --- inaccessible\n",
    "        \n",
    "        # material, care advice and Is-imported\n",
    "        [tshirt_info['materials'], tshirt_info['care_advice'], tshirt_info['origin']] = soup_product.find(\"ul\", class_=\"productSpecification\").get_text().split('\\n')[1:4]\n",
    "        \n",
    "        # Description of product features and highlights\n",
    "        tshirt_info['summary'] = soup_product.find(id=\"collapsible-details-1\").find(\"p\").get_text()\n",
    "        product_details = []\n",
    "        for detail in soup_product.find(id=\"collapsible-details-1\").find_all(\"li\"):\n",
    "            product_details.append(detail.get_text())\n",
    "        tshirt_info['details'] = product_details   \n",
    "        \n",
    "        tshirt_info['details'] = str(datetime.now())\n",
    "        \n",
    "        # add the current product into the t-shirt information list\n",
    "        # tshirts_info_women.append(tshirt_info)  # BUGS!!!!!\n",
    "        \n",
    "        # write the current product info into json output file\n",
    "        print('=======' + tshirt_info['category'] + '-' + tshirt_info['name'] + '=======')\n",
    "        with open('tshirts_kids_uniqlo.json', 'a') as outfile:\n",
    "            json.dump(tshirt_info, outfile)\n",
    "        \n",
    "        # add time span of 2 seconds between each access to the main page of a single product       \n",
    "        time.sleep(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1156"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the dictionary of t-shirt list into pd-dataframe        \n",
    "tshirts_women_df = pd.DataFrame(tshirts_info_women)\n",
    "tshirts_men_df = pd.DataFrame(tshirts_info_men)\n",
    "tshirts_kids_df = pd.DataFrame(tshirts_info_kids)\n",
    "\n",
    "# save all the t-shirts information into a csv file\n",
    "tshirts_all_uniqlo = pd.concat([tshirts_women_df, tshirts_men_df, tshirts_kids_df], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(tshirts_info_women)\n",
    "len(tshirts_info_women)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
